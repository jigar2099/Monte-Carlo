{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f12bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important;}</style>\"))\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "#from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f1f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 1000, 1]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0,1000,10000)\n",
    "x = torch.reshape(x,(10,1,1000,1))\n",
    "y = torch.linspace(0,100,100)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dca920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48330def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 100, 1])\n",
      "Patch shape :  torch.Size([100, 11, 10])\n",
      "MultiHead Output :  torch.Size([100, 11, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 11, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(0,1000,10000)\n",
    "x = torch.reshape(x,(100,1,100,1))\n",
    "y = torch.linspace(0,100,100)\n",
    "print(x.shape)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels:int=1, patch_size=10, emb_size: int=10, img_size: int = 1):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "        # break-down the image in s1Xs2 patches and flat them\n",
    "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size,s2=1),\n",
    "            nn.Linear(patch_size * 1 * in_channels, emb_size)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x\n",
    "\n",
    "# MultiHeadAttention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 10, num_heads: int = 2, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        self.scaling = (self.emb_size // num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        #print('query : ',queries.shape)\n",
    "        #print(n)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        #print('key : ',keys.shape)\n",
    "        values  = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        #print('values : ',values.shape)\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        #print('energy : ',energy.shape)\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        att = F.softmax(energy, dim=-1) * self.scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "'''\n",
    "Whatever goes inside of class, it should have the form of (b,c,h,w)\n",
    "'''\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "print('Patch shape : ',patches_embedded.shape)\n",
    "MultiHeadAttention()(patches_embedded).shape\n",
    "print('MultiHead Output : ', MultiHeadAttention()(patches_embedded).shape)\n",
    "\n",
    "\n",
    "# Residual\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x  = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "# FC Block\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.2):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "# Transformer encoder block\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 10,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098b7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 48, n_classes: int = 1):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 1,\n",
    "                patch_size: int = 10,\n",
    "                emb_size: int = 10,\n",
    "                img_size: int = 1,\n",
    "                depth: int = 10,\n",
    "                n_classes: int = 500,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "        \n",
    "model = ViT()\n",
    "#summary(model, (1, 100, 1), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb7a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1               [-1, 50, 10]               0\n",
      "            Linear-2               [-1, 50, 10]             110\n",
      "    PatchEmbedding-3               [-1, 51, 10]               0\n",
      "         LayerNorm-4               [-1, 51, 10]              20\n",
      "            Linear-5               [-1, 51, 10]             110\n",
      "            Linear-6               [-1, 51, 10]             110\n",
      "            Linear-7               [-1, 51, 10]             110\n",
      "           Dropout-8            [-1, 2, 51, 51]               0\n",
      "            Linear-9               [-1, 51, 10]             110\n",
      "MultiHeadAttention-10               [-1, 51, 10]               0\n",
      "          Dropout-11               [-1, 51, 10]               0\n",
      "      ResidualAdd-12               [-1, 51, 10]               0\n",
      "        LayerNorm-13               [-1, 51, 10]              20\n",
      "           Linear-14               [-1, 51, 40]             440\n",
      "             ReLU-15               [-1, 51, 40]               0\n",
      "          Dropout-16               [-1, 51, 40]               0\n",
      "           Linear-17               [-1, 51, 10]             410\n",
      "          Dropout-18               [-1, 51, 10]               0\n",
      "      ResidualAdd-19               [-1, 51, 10]               0\n",
      "        LayerNorm-20               [-1, 51, 10]              20\n",
      "           Linear-21               [-1, 51, 10]             110\n",
      "           Linear-22               [-1, 51, 10]             110\n",
      "           Linear-23               [-1, 51, 10]             110\n",
      "          Dropout-24            [-1, 2, 51, 51]               0\n",
      "           Linear-25               [-1, 51, 10]             110\n",
      "MultiHeadAttention-26               [-1, 51, 10]               0\n",
      "          Dropout-27               [-1, 51, 10]               0\n",
      "      ResidualAdd-28               [-1, 51, 10]               0\n",
      "        LayerNorm-29               [-1, 51, 10]              20\n",
      "           Linear-30               [-1, 51, 40]             440\n",
      "             ReLU-31               [-1, 51, 40]               0\n",
      "          Dropout-32               [-1, 51, 40]               0\n",
      "           Linear-33               [-1, 51, 10]             410\n",
      "          Dropout-34               [-1, 51, 10]               0\n",
      "      ResidualAdd-35               [-1, 51, 10]               0\n",
      "        LayerNorm-36               [-1, 51, 10]              20\n",
      "           Linear-37               [-1, 51, 10]             110\n",
      "           Linear-38               [-1, 51, 10]             110\n",
      "           Linear-39               [-1, 51, 10]             110\n",
      "          Dropout-40            [-1, 2, 51, 51]               0\n",
      "           Linear-41               [-1, 51, 10]             110\n",
      "MultiHeadAttention-42               [-1, 51, 10]               0\n",
      "          Dropout-43               [-1, 51, 10]               0\n",
      "      ResidualAdd-44               [-1, 51, 10]               0\n",
      "        LayerNorm-45               [-1, 51, 10]              20\n",
      "           Linear-46               [-1, 51, 40]             440\n",
      "             ReLU-47               [-1, 51, 40]               0\n",
      "          Dropout-48               [-1, 51, 40]               0\n",
      "           Linear-49               [-1, 51, 10]             410\n",
      "          Dropout-50               [-1, 51, 10]               0\n",
      "      ResidualAdd-51               [-1, 51, 10]               0\n",
      "        LayerNorm-52               [-1, 51, 10]              20\n",
      "           Linear-53               [-1, 51, 10]             110\n",
      "           Linear-54               [-1, 51, 10]             110\n",
      "           Linear-55               [-1, 51, 10]             110\n",
      "          Dropout-56            [-1, 2, 51, 51]               0\n",
      "           Linear-57               [-1, 51, 10]             110\n",
      "MultiHeadAttention-58               [-1, 51, 10]               0\n",
      "          Dropout-59               [-1, 51, 10]               0\n",
      "      ResidualAdd-60               [-1, 51, 10]               0\n",
      "        LayerNorm-61               [-1, 51, 10]              20\n",
      "           Linear-62               [-1, 51, 40]             440\n",
      "             ReLU-63               [-1, 51, 40]               0\n",
      "          Dropout-64               [-1, 51, 40]               0\n",
      "           Linear-65               [-1, 51, 10]             410\n",
      "          Dropout-66               [-1, 51, 10]               0\n",
      "      ResidualAdd-67               [-1, 51, 10]               0\n",
      "        LayerNorm-68               [-1, 51, 10]              20\n",
      "           Linear-69               [-1, 51, 10]             110\n",
      "           Linear-70               [-1, 51, 10]             110\n",
      "           Linear-71               [-1, 51, 10]             110\n",
      "          Dropout-72            [-1, 2, 51, 51]               0\n",
      "           Linear-73               [-1, 51, 10]             110\n",
      "MultiHeadAttention-74               [-1, 51, 10]               0\n",
      "          Dropout-75               [-1, 51, 10]               0\n",
      "      ResidualAdd-76               [-1, 51, 10]               0\n",
      "        LayerNorm-77               [-1, 51, 10]              20\n",
      "           Linear-78               [-1, 51, 40]             440\n",
      "             ReLU-79               [-1, 51, 40]               0\n",
      "          Dropout-80               [-1, 51, 40]               0\n",
      "           Linear-81               [-1, 51, 10]             410\n",
      "          Dropout-82               [-1, 51, 10]               0\n",
      "      ResidualAdd-83               [-1, 51, 10]               0\n",
      "        LayerNorm-84               [-1, 51, 10]              20\n",
      "           Linear-85               [-1, 51, 10]             110\n",
      "           Linear-86               [-1, 51, 10]             110\n",
      "           Linear-87               [-1, 51, 10]             110\n",
      "          Dropout-88            [-1, 2, 51, 51]               0\n",
      "           Linear-89               [-1, 51, 10]             110\n",
      "MultiHeadAttention-90               [-1, 51, 10]               0\n",
      "          Dropout-91               [-1, 51, 10]               0\n",
      "      ResidualAdd-92               [-1, 51, 10]               0\n",
      "        LayerNorm-93               [-1, 51, 10]              20\n",
      "           Linear-94               [-1, 51, 40]             440\n",
      "             ReLU-95               [-1, 51, 40]               0\n",
      "          Dropout-96               [-1, 51, 40]               0\n",
      "           Linear-97               [-1, 51, 10]             410\n",
      "          Dropout-98               [-1, 51, 10]               0\n",
      "      ResidualAdd-99               [-1, 51, 10]               0\n",
      "       LayerNorm-100               [-1, 51, 10]              20\n",
      "          Linear-101               [-1, 51, 10]             110\n",
      "          Linear-102               [-1, 51, 10]             110\n",
      "          Linear-103               [-1, 51, 10]             110\n",
      "         Dropout-104            [-1, 2, 51, 51]               0\n",
      "          Linear-105               [-1, 51, 10]             110\n",
      "MultiHeadAttention-106               [-1, 51, 10]               0\n",
      "         Dropout-107               [-1, 51, 10]               0\n",
      "     ResidualAdd-108               [-1, 51, 10]               0\n",
      "       LayerNorm-109               [-1, 51, 10]              20\n",
      "          Linear-110               [-1, 51, 40]             440\n",
      "            ReLU-111               [-1, 51, 40]               0\n",
      "         Dropout-112               [-1, 51, 40]               0\n",
      "          Linear-113               [-1, 51, 10]             410\n",
      "         Dropout-114               [-1, 51, 10]               0\n",
      "     ResidualAdd-115               [-1, 51, 10]               0\n",
      "       LayerNorm-116               [-1, 51, 10]              20\n",
      "          Linear-117               [-1, 51, 10]             110\n",
      "          Linear-118               [-1, 51, 10]             110\n",
      "          Linear-119               [-1, 51, 10]             110\n",
      "         Dropout-120            [-1, 2, 51, 51]               0\n",
      "          Linear-121               [-1, 51, 10]             110\n",
      "MultiHeadAttention-122               [-1, 51, 10]               0\n",
      "         Dropout-123               [-1, 51, 10]               0\n",
      "     ResidualAdd-124               [-1, 51, 10]               0\n",
      "       LayerNorm-125               [-1, 51, 10]              20\n",
      "          Linear-126               [-1, 51, 40]             440\n",
      "            ReLU-127               [-1, 51, 40]               0\n",
      "         Dropout-128               [-1, 51, 40]               0\n",
      "          Linear-129               [-1, 51, 10]             410\n",
      "         Dropout-130               [-1, 51, 10]               0\n",
      "     ResidualAdd-131               [-1, 51, 10]               0\n",
      "       LayerNorm-132               [-1, 51, 10]              20\n",
      "          Linear-133               [-1, 51, 10]             110\n",
      "          Linear-134               [-1, 51, 10]             110\n",
      "          Linear-135               [-1, 51, 10]             110\n",
      "         Dropout-136            [-1, 2, 51, 51]               0\n",
      "          Linear-137               [-1, 51, 10]             110\n",
      "MultiHeadAttention-138               [-1, 51, 10]               0\n",
      "         Dropout-139               [-1, 51, 10]               0\n",
      "     ResidualAdd-140               [-1, 51, 10]               0\n",
      "       LayerNorm-141               [-1, 51, 10]              20\n",
      "          Linear-142               [-1, 51, 40]             440\n",
      "            ReLU-143               [-1, 51, 40]               0\n",
      "         Dropout-144               [-1, 51, 40]               0\n",
      "          Linear-145               [-1, 51, 10]             410\n",
      "         Dropout-146               [-1, 51, 10]               0\n",
      "     ResidualAdd-147               [-1, 51, 10]               0\n",
      "       LayerNorm-148               [-1, 51, 10]              20\n",
      "          Linear-149               [-1, 51, 10]             110\n",
      "          Linear-150               [-1, 51, 10]             110\n",
      "          Linear-151               [-1, 51, 10]             110\n",
      "         Dropout-152            [-1, 2, 51, 51]               0\n",
      "          Linear-153               [-1, 51, 10]             110\n",
      "MultiHeadAttention-154               [-1, 51, 10]               0\n",
      "         Dropout-155               [-1, 51, 10]               0\n",
      "     ResidualAdd-156               [-1, 51, 10]               0\n",
      "       LayerNorm-157               [-1, 51, 10]              20\n",
      "          Linear-158               [-1, 51, 40]             440\n",
      "            ReLU-159               [-1, 51, 40]               0\n",
      "         Dropout-160               [-1, 51, 40]               0\n",
      "          Linear-161               [-1, 51, 10]             410\n",
      "         Dropout-162               [-1, 51, 10]               0\n",
      "     ResidualAdd-163               [-1, 51, 10]               0\n",
      "          Reduce-164                   [-1, 10]               0\n",
      "       LayerNorm-165                   [-1, 10]              20\n",
      "          Linear-166                  [-1, 500]           5,500\n",
      "================================================================\n",
      "Total params: 18,930\n",
      "Trainable params: 18,930\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.35\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 1.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ViT(), (1, 500, 1), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5340c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples : 120000000\n",
      "validation samples : 40000000\n",
      "testing samples : 40000000\n"
     ]
    }
   ],
   "source": [
    "train_X = np.load('train_X.npy').flatten()\n",
    "train_y_r = np.load('train_Y.npy').flatten()\n",
    "\n",
    "test_X = np.load('test_X.npy').flatten()\n",
    "test_y_r = np.load('test_y.npy').flatten()\n",
    "\n",
    "val_X = np.load('val_X.npy').flatten()\n",
    "val_y_r = np.load('val_y.npy').flatten()\n",
    "\n",
    "# training\n",
    "#train_X = train_X.reshape(1,np.int(trai  n_X.shape[0]/1000),1000, 1)\n",
    "#train_y_r = train_y_r.reshape(train_y_r.shape[0],1)\n",
    "#train_y_pp = train_y_pp.reshape(np.int(train_y_pp.shape[0]/100),100,1)\n",
    "\n",
    "# validation\n",
    "#val_X = val_X.reshape(np.int(val_X.shape[0]/1000),1000, 1)\n",
    "#val_y_r = val_y_r.reshape(val_y_r.shape[0],1)\n",
    "#val_y_pp = val_y_pp.reshape(np.int(val_y_pp.shape[0]/100),100,1)\n",
    "\n",
    "print(\"training samples :\",train_X.shape[0])\n",
    "print(\"validation samples :\",val_X.shape[0])\n",
    "print(\"testing samples :\",test_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c0f100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "x = torch.from_numpy(train_X[:10000000].astype(np.float32))#.double()\n",
    "x = torch.reshape(x,(10000,1,1000,1))#.double()\n",
    "y = torch.from_numpy(train_y_r[:10000].astype(np.float32))\n",
    "c = 1; h=1000; w=1\n",
    "x_train = torch.Tensor(x).reshape(x.shape[0],c,h,w)\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "vx = torch.from_numpy(val_X[:8000000].astype(np.float32))#.double()\n",
    "vx = torch.reshape(vx,(8000,1,1000,1))#.double()\n",
    "vy = torch.from_numpy(train_y_r[:8000].astype(np.float32))\n",
    "c = 1; h=1000; w=1\n",
    "v_train = torch.Tensor(vx).reshape(vx.shape[0],c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9609a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "x = torch.from_numpy(train_X[:].astype(np.float32))#.double()\n",
    "x = torch.reshape(x,(240000,1,500,1))#.double()\n",
    "#y = torch.from_numpy(train_y_r[:].astype(np.float32))\n",
    "y = torch.from_numpy(train_y_r[:].astype(np.float32))#.double()\n",
    "y = torch.reshape(x,(240000,1,500,1))#.double()\n",
    "c = 1; h=500; w=1\n",
    "x_train = torch.Tensor(x).reshape(x.shape[0],c,h,w)\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "vx = torch.from_numpy(val_X[:].astype(np.float32))#.double()\n",
    "vx = torch.reshape(vx,(80000,1,500,1))#.double()\n",
    "#vy = torch.from_numpy(val_y_r[:].astype(np.float32))\n",
    "vy = torch.from_numpy(val_y_r[:].astype(np.float32))#.double()\n",
    "vy = torch.reshape(vx,(80000,1,500,1))#.double()\n",
    "c = 1; h=500; w=1\n",
    "v_train = torch.Tensor(vx).reshape(vx.shape[0],c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e53181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240000, 1, 500, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371a9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([240000, 1, 500, 1]) - y_train: torch.Size([240000, 500])\n",
      "x_train shape: torch.Size([80000, 1, 500, 1]) - y_train: torch.Size([80000, 500])\n",
      "Epoch: 1/50.. Training loss: 2890.372488088445.. Validation Loss: 1839.591668320921\n",
      "Epoch: 2/50.. Training loss: 1252.486171083918.. Validation Loss: 1096.7007181530182\n",
      "Epoch: 3/50.. Training loss: 896.5267722774416.. Validation Loss: 1061.0430467471529\n",
      "Epoch: 4/50.. Training loss: 850.7629020648725.. Validation Loss: 1015.2399986940451\n",
      "Epoch: 5/50.. Training loss: 844.2745424113803.. Validation Loss: 946.7302320452925\n",
      "Epoch: 6/50.. Training loss: 843.1303900686154.. Validation Loss: 931.8690054911775\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8696/2774896114.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# update the model parameters by performing a single optimization step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# accumulate the training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    148\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    151\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    205\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = torch.Tensor(y).reshape(y.shape[0],y.shape[2])#.type(torch.LongTensor)\n",
    "vy_train = torch.Tensor(vy).reshape(vy.shape[0],vy.shape[2])#.type(torch.LongTensor)\n",
    "#y_test = torch.Tensor(y_test).reshape(y_test.shape[0],)\n",
    "print(f'x_train shape: {x_train.shape} - y_train: {y_train.shape}')\n",
    "print(f'x_train shape: {v_train.shape} - y_train: {vy_train.shape}')\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set,batch_size=256)\n",
    "\n",
    "valid_set = TensorDataset(v_train,vy_train)\n",
    "validloader = DataLoader(valid_set,batch_size=256)\n",
    "\n",
    "from torch import optim\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay= 1e-3, momentum = 0.6, nesterov = True)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "n_epochs = 50 # this is a hyperparameter you'll need to define\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##################\n",
    "    ### TRAIN LOOP ###\n",
    "    ##################\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the old gradients from optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: feed inputs to the model to get outputs\n",
    "        output = model(data)\n",
    "        # calculate the training batch loss\n",
    "        loss = loss_function(output, target)\n",
    "        # backward: perform gradient descent of the loss w.r. to the model params\n",
    "        loss.backward()\n",
    "        # update the model parameters by performing a single optimization step\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #######################\n",
    "    ### VALIDATION LOOP ###\n",
    "    #######################\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for data, target in validloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # validation batch loss\n",
    "            loss = loss_function(output, target) \n",
    "            # accumulate the valid_loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    #########################\n",
    "    ## PRINT EPOCH RESULTS ##\n",
    "    #########################\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validloader)\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643ff05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3db2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fd5f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([161000, 1, 1000, 1]) - y_train: torch.Size([161000])\n",
      "x_train shape: torch.Size([8000, 1, 1000, 1]) - y_train: torch.Size([8000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jigar\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/1953217474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# forward pass: feed inputs to the model to get outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m# calculate the training batch loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/2857984585.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mx\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/2857984585.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b n (h d) -> b h n d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#print('key : ',keys.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mvalues\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b n (h d) -> b h n d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m#print('values : ',values.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# sum up over the last axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = torch.Tensor(y).reshape(y.shape[0],)#.type(torch.LongTensor)\n",
    "vy_train = torch.Tensor(vy).reshape(vy.shape[0],)#.type(torch.LongTensor)\n",
    "#y_test = torch.Tensor(y_test).reshape(y_test.shape[0],)\n",
    "print(f'x_train shape: {x_train.shape} - y_train: {y_train.shape}')\n",
    "print(f'x_train shape: {v_train.shape} - y_train: {vy_train.shape}')\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set,batch_size=1000)\n",
    "\n",
    "valid_set = TensorDataset(v_train,vy_train)\n",
    "validloader = DataLoader(valid_set,batch_size=1000)\n",
    "\n",
    "from torch import optim\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay= 1e-3, momentum = 0.6, nesterov = True)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "n_epochs = 50 # this is a hyperparameter you'll need to define\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##################\n",
    "    ### TRAIN LOOP ###\n",
    "    ##################\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        # clear the old gradients from optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: feed inputs to the model to get outputs\n",
    "        output = model(data)\n",
    "        # calculate the training batch loss\n",
    "        loss = loss_function(output, target)\n",
    "        # backward: perform gradient descent of the loss w.r. to the model params\n",
    "        loss.backward()\n",
    "        # update the model parameters by performing a single optimization step\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #######################\n",
    "    ### VALIDATION LOOP ###\n",
    "    #######################\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for data, target in validloader:\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # validation batch loss\n",
    "            loss = loss_function(output, target) \n",
    "            # accumulate the valid_loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    #########################\n",
    "    ## PRINT EPOCH RESULTS ##\n",
    "    #########################\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validloader)\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad8577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937bd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba1a6585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.cuda.device_count()\n",
    "\n",
    "torch.cuda.current_device()\n",
    "\n",
    "torch.cuda.device(0)\n",
    "\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2f9945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f000a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b663505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ac5aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([161000, 1, 1000, 1]) - y_train: torch.Size([161000])\n",
      "x_train shape: torch.Size([8000, 1, 1000, 1]) - y_train: torch.Size([8000])\n",
      "Epoch: 1/50.. Training loss: 6461.130845883248.. Validation Loss: 362.01186180114746\n",
      "Epoch: 2/50.. Training loss: 4439.795080212332.. Validation Loss: 1370.8190307617188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/3640602335.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# update the model parameters by performing a single optimization step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# accumulate the training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    148\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    151\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    205\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mstep_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# update step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mstep_t\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = torch.Tensor(y).reshape(y.shape[0],)#.type(torch.LongTensor)\n",
    "vy_train = torch.Tensor(vy).reshape(vy.shape[0],)#.type(torch.LongTensor)\n",
    "#y_test = torch.Tensor(y_test).reshape(y_test.shape[0],)\n",
    "print(f'x_train shape: {x_train.shape} - y_train: {y_train.shape}')\n",
    "print(f'x_train shape: {v_train.shape} - y_train: {vy_train.shape}')\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set,batch_size=1000)\n",
    "\n",
    "valid_set = TensorDataset(v_train,vy_train)\n",
    "validloader = DataLoader(valid_set,batch_size=1000)\n",
    "\n",
    "from torch import optim\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay= 1e-3, momentum = 0.6, nesterov = True)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "n_epochs = 50 # this is a hyperparameter you'll need to define\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##################\n",
    "    ### TRAIN LOOP ###\n",
    "    ##################\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the old gradients from optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: feed inputs to the model to get outputs\n",
    "        output = model(data)\n",
    "        # calculate the training batch loss\n",
    "        loss = loss_function(output, target)\n",
    "        # backward: perform gradient descent of the loss w.r. to the model params\n",
    "        loss.backward()\n",
    "        # update the model parameters by performing a single optimization step\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #######################\n",
    "    ### VALIDATION LOOP ###\n",
    "    #######################\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for data, target in validloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # validation batch loss\n",
    "            loss = loss_function(output, target) \n",
    "            # accumulate the valid_loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    #########################\n",
    "    ## PRINT EPOCH RESULTS ##\n",
    "    #########################\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validloader)\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18776bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
