{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f12bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important;}</style>\"))\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "#from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f1f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 1000, 1]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0,1000,10000)\n",
    "x = torch.reshape(x,(10,1,1000,1))\n",
    "y = torch.linspace(0,100,100)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dca920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48330def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 100, 1])\n",
      "Patch shape :  torch.Size([100, 11, 10])\n",
      "MultiHead Output :  torch.Size([100, 11, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 11, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(0,1000,10000)\n",
    "x = torch.reshape(x,(100,1,100,1))\n",
    "y = torch.linspace(0,100,100)\n",
    "print(x.shape)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels:int=1, patch_size=10, emb_size: int=10, img_size: int = 1):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "        # break-down the image in s1Xs2 patches and flat them\n",
    "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size,s2=1),\n",
    "            nn.Linear(patch_size * 1 * in_channels, emb_size)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x\n",
    "\n",
    "# MultiHeadAttention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 10, num_heads: int = 2, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        self.scaling = (self.emb_size // num_heads) ** -0.5\n",
    "\n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        #print('query : ',queries.shape)\n",
    "        #print(n)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        #print('key : ',keys.shape)\n",
    "        values  = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        #print('values : ',values.shape)\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        #print('energy : ',energy.shape)\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        att = F.softmax(energy, dim=-1) * self.scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "'''\n",
    "Whatever goes inside of class, it should have the form of (b,c,h,w)\n",
    "'''\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "print('Patch shape : ',patches_embedded.shape)\n",
    "MultiHeadAttention()(patches_embedded).shape\n",
    "print('MultiHead Output : ', MultiHeadAttention()(patches_embedded).shape)\n",
    "\n",
    "\n",
    "# Residual\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x  = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "# FC Block\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.2):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "# Transformer encoder block\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 10,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 50,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "098b7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 48, n_classes: int = 1):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 1,\n",
    "                patch_size: int = 10,\n",
    "                emb_size: int = 10,\n",
    "                img_size: int = 1,\n",
    "                depth: int = 50,\n",
    "                n_classes: int = 1,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "        \n",
    "model = ViT()\n",
    "#summary(model, (1, 100, 1), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bb7a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1               [-1, 50, 10]               0\n",
      "            Linear-2               [-1, 50, 10]             110\n",
      "    PatchEmbedding-3               [-1, 51, 10]               0\n",
      "         LayerNorm-4               [-1, 51, 10]              20\n",
      "            Linear-5               [-1, 51, 10]             110\n",
      "            Linear-6               [-1, 51, 10]             110\n",
      "            Linear-7               [-1, 51, 10]             110\n",
      "           Dropout-8            [-1, 2, 51, 51]               0\n",
      "            Linear-9               [-1, 51, 10]             110\n",
      "MultiHeadAttention-10               [-1, 51, 10]               0\n",
      "          Dropout-11               [-1, 51, 10]               0\n",
      "      ResidualAdd-12               [-1, 51, 10]               0\n",
      "        LayerNorm-13               [-1, 51, 10]              20\n",
      "           Linear-14              [-1, 51, 500]           5,500\n",
      "             ReLU-15              [-1, 51, 500]               0\n",
      "          Dropout-16              [-1, 51, 500]               0\n",
      "           Linear-17               [-1, 51, 10]           5,010\n",
      "          Dropout-18               [-1, 51, 10]               0\n",
      "      ResidualAdd-19               [-1, 51, 10]               0\n",
      "        LayerNorm-20               [-1, 51, 10]              20\n",
      "           Linear-21               [-1, 51, 10]             110\n",
      "           Linear-22               [-1, 51, 10]             110\n",
      "           Linear-23               [-1, 51, 10]             110\n",
      "          Dropout-24            [-1, 2, 51, 51]               0\n",
      "           Linear-25               [-1, 51, 10]             110\n",
      "MultiHeadAttention-26               [-1, 51, 10]               0\n",
      "          Dropout-27               [-1, 51, 10]               0\n",
      "      ResidualAdd-28               [-1, 51, 10]               0\n",
      "        LayerNorm-29               [-1, 51, 10]              20\n",
      "           Linear-30              [-1, 51, 500]           5,500\n",
      "             ReLU-31              [-1, 51, 500]               0\n",
      "          Dropout-32              [-1, 51, 500]               0\n",
      "           Linear-33               [-1, 51, 10]           5,010\n",
      "          Dropout-34               [-1, 51, 10]               0\n",
      "      ResidualAdd-35               [-1, 51, 10]               0\n",
      "        LayerNorm-36               [-1, 51, 10]              20\n",
      "           Linear-37               [-1, 51, 10]             110\n",
      "           Linear-38               [-1, 51, 10]             110\n",
      "           Linear-39               [-1, 51, 10]             110\n",
      "          Dropout-40            [-1, 2, 51, 51]               0\n",
      "           Linear-41               [-1, 51, 10]             110\n",
      "MultiHeadAttention-42               [-1, 51, 10]               0\n",
      "          Dropout-43               [-1, 51, 10]               0\n",
      "      ResidualAdd-44               [-1, 51, 10]               0\n",
      "        LayerNorm-45               [-1, 51, 10]              20\n",
      "           Linear-46              [-1, 51, 500]           5,500\n",
      "             ReLU-47              [-1, 51, 500]               0\n",
      "          Dropout-48              [-1, 51, 500]               0\n",
      "           Linear-49               [-1, 51, 10]           5,010\n",
      "          Dropout-50               [-1, 51, 10]               0\n",
      "      ResidualAdd-51               [-1, 51, 10]               0\n",
      "        LayerNorm-52               [-1, 51, 10]              20\n",
      "           Linear-53               [-1, 51, 10]             110\n",
      "           Linear-54               [-1, 51, 10]             110\n",
      "           Linear-55               [-1, 51, 10]             110\n",
      "          Dropout-56            [-1, 2, 51, 51]               0\n",
      "           Linear-57               [-1, 51, 10]             110\n",
      "MultiHeadAttention-58               [-1, 51, 10]               0\n",
      "          Dropout-59               [-1, 51, 10]               0\n",
      "      ResidualAdd-60               [-1, 51, 10]               0\n",
      "        LayerNorm-61               [-1, 51, 10]              20\n",
      "           Linear-62              [-1, 51, 500]           5,500\n",
      "             ReLU-63              [-1, 51, 500]               0\n",
      "          Dropout-64              [-1, 51, 500]               0\n",
      "           Linear-65               [-1, 51, 10]           5,010\n",
      "          Dropout-66               [-1, 51, 10]               0\n",
      "      ResidualAdd-67               [-1, 51, 10]               0\n",
      "        LayerNorm-68               [-1, 51, 10]              20\n",
      "           Linear-69               [-1, 51, 10]             110\n",
      "           Linear-70               [-1, 51, 10]             110\n",
      "           Linear-71               [-1, 51, 10]             110\n",
      "          Dropout-72            [-1, 2, 51, 51]               0\n",
      "           Linear-73               [-1, 51, 10]             110\n",
      "MultiHeadAttention-74               [-1, 51, 10]               0\n",
      "          Dropout-75               [-1, 51, 10]               0\n",
      "      ResidualAdd-76               [-1, 51, 10]               0\n",
      "        LayerNorm-77               [-1, 51, 10]              20\n",
      "           Linear-78              [-1, 51, 500]           5,500\n",
      "             ReLU-79              [-1, 51, 500]               0\n",
      "          Dropout-80              [-1, 51, 500]               0\n",
      "           Linear-81               [-1, 51, 10]           5,010\n",
      "          Dropout-82               [-1, 51, 10]               0\n",
      "      ResidualAdd-83               [-1, 51, 10]               0\n",
      "        LayerNorm-84               [-1, 51, 10]              20\n",
      "           Linear-85               [-1, 51, 10]             110\n",
      "           Linear-86               [-1, 51, 10]             110\n",
      "           Linear-87               [-1, 51, 10]             110\n",
      "          Dropout-88            [-1, 2, 51, 51]               0\n",
      "           Linear-89               [-1, 51, 10]             110\n",
      "MultiHeadAttention-90               [-1, 51, 10]               0\n",
      "          Dropout-91               [-1, 51, 10]               0\n",
      "      ResidualAdd-92               [-1, 51, 10]               0\n",
      "        LayerNorm-93               [-1, 51, 10]              20\n",
      "           Linear-94              [-1, 51, 500]           5,500\n",
      "             ReLU-95              [-1, 51, 500]               0\n",
      "          Dropout-96              [-1, 51, 500]               0\n",
      "           Linear-97               [-1, 51, 10]           5,010\n",
      "          Dropout-98               [-1, 51, 10]               0\n",
      "      ResidualAdd-99               [-1, 51, 10]               0\n",
      "       LayerNorm-100               [-1, 51, 10]              20\n",
      "          Linear-101               [-1, 51, 10]             110\n",
      "          Linear-102               [-1, 51, 10]             110\n",
      "          Linear-103               [-1, 51, 10]             110\n",
      "         Dropout-104            [-1, 2, 51, 51]               0\n",
      "          Linear-105               [-1, 51, 10]             110\n",
      "MultiHeadAttention-106               [-1, 51, 10]               0\n",
      "         Dropout-107               [-1, 51, 10]               0\n",
      "     ResidualAdd-108               [-1, 51, 10]               0\n",
      "       LayerNorm-109               [-1, 51, 10]              20\n",
      "          Linear-110              [-1, 51, 500]           5,500\n",
      "            ReLU-111              [-1, 51, 500]               0\n",
      "         Dropout-112              [-1, 51, 500]               0\n",
      "          Linear-113               [-1, 51, 10]           5,010\n",
      "         Dropout-114               [-1, 51, 10]               0\n",
      "     ResidualAdd-115               [-1, 51, 10]               0\n",
      "       LayerNorm-116               [-1, 51, 10]              20\n",
      "          Linear-117               [-1, 51, 10]             110\n",
      "          Linear-118               [-1, 51, 10]             110\n",
      "          Linear-119               [-1, 51, 10]             110\n",
      "         Dropout-120            [-1, 2, 51, 51]               0\n",
      "          Linear-121               [-1, 51, 10]             110\n",
      "MultiHeadAttention-122               [-1, 51, 10]               0\n",
      "         Dropout-123               [-1, 51, 10]               0\n",
      "     ResidualAdd-124               [-1, 51, 10]               0\n",
      "       LayerNorm-125               [-1, 51, 10]              20\n",
      "          Linear-126              [-1, 51, 500]           5,500\n",
      "            ReLU-127              [-1, 51, 500]               0\n",
      "         Dropout-128              [-1, 51, 500]               0\n",
      "          Linear-129               [-1, 51, 10]           5,010\n",
      "         Dropout-130               [-1, 51, 10]               0\n",
      "     ResidualAdd-131               [-1, 51, 10]               0\n",
      "       LayerNorm-132               [-1, 51, 10]              20\n",
      "          Linear-133               [-1, 51, 10]             110\n",
      "          Linear-134               [-1, 51, 10]             110\n",
      "          Linear-135               [-1, 51, 10]             110\n",
      "         Dropout-136            [-1, 2, 51, 51]               0\n",
      "          Linear-137               [-1, 51, 10]             110\n",
      "MultiHeadAttention-138               [-1, 51, 10]               0\n",
      "         Dropout-139               [-1, 51, 10]               0\n",
      "     ResidualAdd-140               [-1, 51, 10]               0\n",
      "       LayerNorm-141               [-1, 51, 10]              20\n",
      "          Linear-142              [-1, 51, 500]           5,500\n",
      "            ReLU-143              [-1, 51, 500]               0\n",
      "         Dropout-144              [-1, 51, 500]               0\n",
      "          Linear-145               [-1, 51, 10]           5,010\n",
      "         Dropout-146               [-1, 51, 10]               0\n",
      "     ResidualAdd-147               [-1, 51, 10]               0\n",
      "       LayerNorm-148               [-1, 51, 10]              20\n",
      "          Linear-149               [-1, 51, 10]             110\n",
      "          Linear-150               [-1, 51, 10]             110\n",
      "          Linear-151               [-1, 51, 10]             110\n",
      "         Dropout-152            [-1, 2, 51, 51]               0\n",
      "          Linear-153               [-1, 51, 10]             110\n",
      "MultiHeadAttention-154               [-1, 51, 10]               0\n",
      "         Dropout-155               [-1, 51, 10]               0\n",
      "     ResidualAdd-156               [-1, 51, 10]               0\n",
      "       LayerNorm-157               [-1, 51, 10]              20\n",
      "          Linear-158              [-1, 51, 500]           5,500\n",
      "            ReLU-159              [-1, 51, 500]               0\n",
      "         Dropout-160              [-1, 51, 500]               0\n",
      "          Linear-161               [-1, 51, 10]           5,010\n",
      "         Dropout-162               [-1, 51, 10]               0\n",
      "     ResidualAdd-163               [-1, 51, 10]               0\n",
      "       LayerNorm-164               [-1, 51, 10]              20\n",
      "          Linear-165               [-1, 51, 10]             110\n",
      "          Linear-166               [-1, 51, 10]             110\n",
      "          Linear-167               [-1, 51, 10]             110\n",
      "         Dropout-168            [-1, 2, 51, 51]               0\n",
      "          Linear-169               [-1, 51, 10]             110\n",
      "MultiHeadAttention-170               [-1, 51, 10]               0\n",
      "         Dropout-171               [-1, 51, 10]               0\n",
      "     ResidualAdd-172               [-1, 51, 10]               0\n",
      "       LayerNorm-173               [-1, 51, 10]              20\n",
      "          Linear-174              [-1, 51, 500]           5,500\n",
      "            ReLU-175              [-1, 51, 500]               0\n",
      "         Dropout-176              [-1, 51, 500]               0\n",
      "          Linear-177               [-1, 51, 10]           5,010\n",
      "         Dropout-178               [-1, 51, 10]               0\n",
      "     ResidualAdd-179               [-1, 51, 10]               0\n",
      "       LayerNorm-180               [-1, 51, 10]              20\n",
      "          Linear-181               [-1, 51, 10]             110\n",
      "          Linear-182               [-1, 51, 10]             110\n",
      "          Linear-183               [-1, 51, 10]             110\n",
      "         Dropout-184            [-1, 2, 51, 51]               0\n",
      "          Linear-185               [-1, 51, 10]             110\n",
      "MultiHeadAttention-186               [-1, 51, 10]               0\n",
      "         Dropout-187               [-1, 51, 10]               0\n",
      "     ResidualAdd-188               [-1, 51, 10]               0\n",
      "       LayerNorm-189               [-1, 51, 10]              20\n",
      "          Linear-190              [-1, 51, 500]           5,500\n",
      "            ReLU-191              [-1, 51, 500]               0\n",
      "         Dropout-192              [-1, 51, 500]               0\n",
      "          Linear-193               [-1, 51, 10]           5,010\n",
      "         Dropout-194               [-1, 51, 10]               0\n",
      "     ResidualAdd-195               [-1, 51, 10]               0\n",
      "       LayerNorm-196               [-1, 51, 10]              20\n",
      "          Linear-197               [-1, 51, 10]             110\n",
      "          Linear-198               [-1, 51, 10]             110\n",
      "          Linear-199               [-1, 51, 10]             110\n",
      "         Dropout-200            [-1, 2, 51, 51]               0\n",
      "          Linear-201               [-1, 51, 10]             110\n",
      "MultiHeadAttention-202               [-1, 51, 10]               0\n",
      "         Dropout-203               [-1, 51, 10]               0\n",
      "     ResidualAdd-204               [-1, 51, 10]               0\n",
      "       LayerNorm-205               [-1, 51, 10]              20\n",
      "          Linear-206              [-1, 51, 500]           5,500\n",
      "            ReLU-207              [-1, 51, 500]               0\n",
      "         Dropout-208              [-1, 51, 500]               0\n",
      "          Linear-209               [-1, 51, 10]           5,010\n",
      "         Dropout-210               [-1, 51, 10]               0\n",
      "     ResidualAdd-211               [-1, 51, 10]               0\n",
      "       LayerNorm-212               [-1, 51, 10]              20\n",
      "          Linear-213               [-1, 51, 10]             110\n",
      "          Linear-214               [-1, 51, 10]             110\n",
      "          Linear-215               [-1, 51, 10]             110\n",
      "         Dropout-216            [-1, 2, 51, 51]               0\n",
      "          Linear-217               [-1, 51, 10]             110\n",
      "MultiHeadAttention-218               [-1, 51, 10]               0\n",
      "         Dropout-219               [-1, 51, 10]               0\n",
      "     ResidualAdd-220               [-1, 51, 10]               0\n",
      "       LayerNorm-221               [-1, 51, 10]              20\n",
      "          Linear-222              [-1, 51, 500]           5,500\n",
      "            ReLU-223              [-1, 51, 500]               0\n",
      "         Dropout-224              [-1, 51, 500]               0\n",
      "          Linear-225               [-1, 51, 10]           5,010\n",
      "         Dropout-226               [-1, 51, 10]               0\n",
      "     ResidualAdd-227               [-1, 51, 10]               0\n",
      "       LayerNorm-228               [-1, 51, 10]              20\n",
      "          Linear-229               [-1, 51, 10]             110\n",
      "          Linear-230               [-1, 51, 10]             110\n",
      "          Linear-231               [-1, 51, 10]             110\n",
      "         Dropout-232            [-1, 2, 51, 51]               0\n",
      "          Linear-233               [-1, 51, 10]             110\n",
      "MultiHeadAttention-234               [-1, 51, 10]               0\n",
      "         Dropout-235               [-1, 51, 10]               0\n",
      "     ResidualAdd-236               [-1, 51, 10]               0\n",
      "       LayerNorm-237               [-1, 51, 10]              20\n",
      "          Linear-238              [-1, 51, 500]           5,500\n",
      "            ReLU-239              [-1, 51, 500]               0\n",
      "         Dropout-240              [-1, 51, 500]               0\n",
      "          Linear-241               [-1, 51, 10]           5,010\n",
      "         Dropout-242               [-1, 51, 10]               0\n",
      "     ResidualAdd-243               [-1, 51, 10]               0\n",
      "       LayerNorm-244               [-1, 51, 10]              20\n",
      "          Linear-245               [-1, 51, 10]             110\n",
      "          Linear-246               [-1, 51, 10]             110\n",
      "          Linear-247               [-1, 51, 10]             110\n",
      "         Dropout-248            [-1, 2, 51, 51]               0\n",
      "          Linear-249               [-1, 51, 10]             110\n",
      "MultiHeadAttention-250               [-1, 51, 10]               0\n",
      "         Dropout-251               [-1, 51, 10]               0\n",
      "     ResidualAdd-252               [-1, 51, 10]               0\n",
      "       LayerNorm-253               [-1, 51, 10]              20\n",
      "          Linear-254              [-1, 51, 500]           5,500\n",
      "            ReLU-255              [-1, 51, 500]               0\n",
      "         Dropout-256              [-1, 51, 500]               0\n",
      "          Linear-257               [-1, 51, 10]           5,010\n",
      "         Dropout-258               [-1, 51, 10]               0\n",
      "     ResidualAdd-259               [-1, 51, 10]               0\n",
      "       LayerNorm-260               [-1, 51, 10]              20\n",
      "          Linear-261               [-1, 51, 10]             110\n",
      "          Linear-262               [-1, 51, 10]             110\n",
      "          Linear-263               [-1, 51, 10]             110\n",
      "         Dropout-264            [-1, 2, 51, 51]               0\n",
      "          Linear-265               [-1, 51, 10]             110\n",
      "MultiHeadAttention-266               [-1, 51, 10]               0\n",
      "         Dropout-267               [-1, 51, 10]               0\n",
      "     ResidualAdd-268               [-1, 51, 10]               0\n",
      "       LayerNorm-269               [-1, 51, 10]              20\n",
      "          Linear-270              [-1, 51, 500]           5,500\n",
      "            ReLU-271              [-1, 51, 500]               0\n",
      "         Dropout-272              [-1, 51, 500]               0\n",
      "          Linear-273               [-1, 51, 10]           5,010\n",
      "         Dropout-274               [-1, 51, 10]               0\n",
      "     ResidualAdd-275               [-1, 51, 10]               0\n",
      "       LayerNorm-276               [-1, 51, 10]              20\n",
      "          Linear-277               [-1, 51, 10]             110\n",
      "          Linear-278               [-1, 51, 10]             110\n",
      "          Linear-279               [-1, 51, 10]             110\n",
      "         Dropout-280            [-1, 2, 51, 51]               0\n",
      "          Linear-281               [-1, 51, 10]             110\n",
      "MultiHeadAttention-282               [-1, 51, 10]               0\n",
      "         Dropout-283               [-1, 51, 10]               0\n",
      "     ResidualAdd-284               [-1, 51, 10]               0\n",
      "       LayerNorm-285               [-1, 51, 10]              20\n",
      "          Linear-286              [-1, 51, 500]           5,500\n",
      "            ReLU-287              [-1, 51, 500]               0\n",
      "         Dropout-288              [-1, 51, 500]               0\n",
      "          Linear-289               [-1, 51, 10]           5,010\n",
      "         Dropout-290               [-1, 51, 10]               0\n",
      "     ResidualAdd-291               [-1, 51, 10]               0\n",
      "       LayerNorm-292               [-1, 51, 10]              20\n",
      "          Linear-293               [-1, 51, 10]             110\n",
      "          Linear-294               [-1, 51, 10]             110\n",
      "          Linear-295               [-1, 51, 10]             110\n",
      "         Dropout-296            [-1, 2, 51, 51]               0\n",
      "          Linear-297               [-1, 51, 10]             110\n",
      "MultiHeadAttention-298               [-1, 51, 10]               0\n",
      "         Dropout-299               [-1, 51, 10]               0\n",
      "     ResidualAdd-300               [-1, 51, 10]               0\n",
      "       LayerNorm-301               [-1, 51, 10]              20\n",
      "          Linear-302              [-1, 51, 500]           5,500\n",
      "            ReLU-303              [-1, 51, 500]               0\n",
      "         Dropout-304              [-1, 51, 500]               0\n",
      "          Linear-305               [-1, 51, 10]           5,010\n",
      "         Dropout-306               [-1, 51, 10]               0\n",
      "     ResidualAdd-307               [-1, 51, 10]               0\n",
      "       LayerNorm-308               [-1, 51, 10]              20\n",
      "          Linear-309               [-1, 51, 10]             110\n",
      "          Linear-310               [-1, 51, 10]             110\n",
      "          Linear-311               [-1, 51, 10]             110\n",
      "         Dropout-312            [-1, 2, 51, 51]               0\n",
      "          Linear-313               [-1, 51, 10]             110\n",
      "MultiHeadAttention-314               [-1, 51, 10]               0\n",
      "         Dropout-315               [-1, 51, 10]               0\n",
      "     ResidualAdd-316               [-1, 51, 10]               0\n",
      "       LayerNorm-317               [-1, 51, 10]              20\n",
      "          Linear-318              [-1, 51, 500]           5,500\n",
      "            ReLU-319              [-1, 51, 500]               0\n",
      "         Dropout-320              [-1, 51, 500]               0\n",
      "          Linear-321               [-1, 51, 10]           5,010\n",
      "         Dropout-322               [-1, 51, 10]               0\n",
      "     ResidualAdd-323               [-1, 51, 10]               0\n",
      "       LayerNorm-324               [-1, 51, 10]              20\n",
      "          Linear-325               [-1, 51, 10]             110\n",
      "          Linear-326               [-1, 51, 10]             110\n",
      "          Linear-327               [-1, 51, 10]             110\n",
      "         Dropout-328            [-1, 2, 51, 51]               0\n",
      "          Linear-329               [-1, 51, 10]             110\n",
      "MultiHeadAttention-330               [-1, 51, 10]               0\n",
      "         Dropout-331               [-1, 51, 10]               0\n",
      "     ResidualAdd-332               [-1, 51, 10]               0\n",
      "       LayerNorm-333               [-1, 51, 10]              20\n",
      "          Linear-334              [-1, 51, 500]           5,500\n",
      "            ReLU-335              [-1, 51, 500]               0\n",
      "         Dropout-336              [-1, 51, 500]               0\n",
      "          Linear-337               [-1, 51, 10]           5,010\n",
      "         Dropout-338               [-1, 51, 10]               0\n",
      "     ResidualAdd-339               [-1, 51, 10]               0\n",
      "       LayerNorm-340               [-1, 51, 10]              20\n",
      "          Linear-341               [-1, 51, 10]             110\n",
      "          Linear-342               [-1, 51, 10]             110\n",
      "          Linear-343               [-1, 51, 10]             110\n",
      "         Dropout-344            [-1, 2, 51, 51]               0\n",
      "          Linear-345               [-1, 51, 10]             110\n",
      "MultiHeadAttention-346               [-1, 51, 10]               0\n",
      "         Dropout-347               [-1, 51, 10]               0\n",
      "     ResidualAdd-348               [-1, 51, 10]               0\n",
      "       LayerNorm-349               [-1, 51, 10]              20\n",
      "          Linear-350              [-1, 51, 500]           5,500\n",
      "            ReLU-351              [-1, 51, 500]               0\n",
      "         Dropout-352              [-1, 51, 500]               0\n",
      "          Linear-353               [-1, 51, 10]           5,010\n",
      "         Dropout-354               [-1, 51, 10]               0\n",
      "     ResidualAdd-355               [-1, 51, 10]               0\n",
      "       LayerNorm-356               [-1, 51, 10]              20\n",
      "          Linear-357               [-1, 51, 10]             110\n",
      "          Linear-358               [-1, 51, 10]             110\n",
      "          Linear-359               [-1, 51, 10]             110\n",
      "         Dropout-360            [-1, 2, 51, 51]               0\n",
      "          Linear-361               [-1, 51, 10]             110\n",
      "MultiHeadAttention-362               [-1, 51, 10]               0\n",
      "         Dropout-363               [-1, 51, 10]               0\n",
      "     ResidualAdd-364               [-1, 51, 10]               0\n",
      "       LayerNorm-365               [-1, 51, 10]              20\n",
      "          Linear-366              [-1, 51, 500]           5,500\n",
      "            ReLU-367              [-1, 51, 500]               0\n",
      "         Dropout-368              [-1, 51, 500]               0\n",
      "          Linear-369               [-1, 51, 10]           5,010\n",
      "         Dropout-370               [-1, 51, 10]               0\n",
      "     ResidualAdd-371               [-1, 51, 10]               0\n",
      "       LayerNorm-372               [-1, 51, 10]              20\n",
      "          Linear-373               [-1, 51, 10]             110\n",
      "          Linear-374               [-1, 51, 10]             110\n",
      "          Linear-375               [-1, 51, 10]             110\n",
      "         Dropout-376            [-1, 2, 51, 51]               0\n",
      "          Linear-377               [-1, 51, 10]             110\n",
      "MultiHeadAttention-378               [-1, 51, 10]               0\n",
      "         Dropout-379               [-1, 51, 10]               0\n",
      "     ResidualAdd-380               [-1, 51, 10]               0\n",
      "       LayerNorm-381               [-1, 51, 10]              20\n",
      "          Linear-382              [-1, 51, 500]           5,500\n",
      "            ReLU-383              [-1, 51, 500]               0\n",
      "         Dropout-384              [-1, 51, 500]               0\n",
      "          Linear-385               [-1, 51, 10]           5,010\n",
      "         Dropout-386               [-1, 51, 10]               0\n",
      "     ResidualAdd-387               [-1, 51, 10]               0\n",
      "       LayerNorm-388               [-1, 51, 10]              20\n",
      "          Linear-389               [-1, 51, 10]             110\n",
      "          Linear-390               [-1, 51, 10]             110\n",
      "          Linear-391               [-1, 51, 10]             110\n",
      "         Dropout-392            [-1, 2, 51, 51]               0\n",
      "          Linear-393               [-1, 51, 10]             110\n",
      "MultiHeadAttention-394               [-1, 51, 10]               0\n",
      "         Dropout-395               [-1, 51, 10]               0\n",
      "     ResidualAdd-396               [-1, 51, 10]               0\n",
      "       LayerNorm-397               [-1, 51, 10]              20\n",
      "          Linear-398              [-1, 51, 500]           5,500\n",
      "            ReLU-399              [-1, 51, 500]               0\n",
      "         Dropout-400              [-1, 51, 500]               0\n",
      "          Linear-401               [-1, 51, 10]           5,010\n",
      "         Dropout-402               [-1, 51, 10]               0\n",
      "     ResidualAdd-403               [-1, 51, 10]               0\n",
      "       LayerNorm-404               [-1, 51, 10]              20\n",
      "          Linear-405               [-1, 51, 10]             110\n",
      "          Linear-406               [-1, 51, 10]             110\n",
      "          Linear-407               [-1, 51, 10]             110\n",
      "         Dropout-408            [-1, 2, 51, 51]               0\n",
      "          Linear-409               [-1, 51, 10]             110\n",
      "MultiHeadAttention-410               [-1, 51, 10]               0\n",
      "         Dropout-411               [-1, 51, 10]               0\n",
      "     ResidualAdd-412               [-1, 51, 10]               0\n",
      "       LayerNorm-413               [-1, 51, 10]              20\n",
      "          Linear-414              [-1, 51, 500]           5,500\n",
      "            ReLU-415              [-1, 51, 500]               0\n",
      "         Dropout-416              [-1, 51, 500]               0\n",
      "          Linear-417               [-1, 51, 10]           5,010\n",
      "         Dropout-418               [-1, 51, 10]               0\n",
      "     ResidualAdd-419               [-1, 51, 10]               0\n",
      "       LayerNorm-420               [-1, 51, 10]              20\n",
      "          Linear-421               [-1, 51, 10]             110\n",
      "          Linear-422               [-1, 51, 10]             110\n",
      "          Linear-423               [-1, 51, 10]             110\n",
      "         Dropout-424            [-1, 2, 51, 51]               0\n",
      "          Linear-425               [-1, 51, 10]             110\n",
      "MultiHeadAttention-426               [-1, 51, 10]               0\n",
      "         Dropout-427               [-1, 51, 10]               0\n",
      "     ResidualAdd-428               [-1, 51, 10]               0\n",
      "       LayerNorm-429               [-1, 51, 10]              20\n",
      "          Linear-430              [-1, 51, 500]           5,500\n",
      "            ReLU-431              [-1, 51, 500]               0\n",
      "         Dropout-432              [-1, 51, 500]               0\n",
      "          Linear-433               [-1, 51, 10]           5,010\n",
      "         Dropout-434               [-1, 51, 10]               0\n",
      "     ResidualAdd-435               [-1, 51, 10]               0\n",
      "       LayerNorm-436               [-1, 51, 10]              20\n",
      "          Linear-437               [-1, 51, 10]             110\n",
      "          Linear-438               [-1, 51, 10]             110\n",
      "          Linear-439               [-1, 51, 10]             110\n",
      "         Dropout-440            [-1, 2, 51, 51]               0\n",
      "          Linear-441               [-1, 51, 10]             110\n",
      "MultiHeadAttention-442               [-1, 51, 10]               0\n",
      "         Dropout-443               [-1, 51, 10]               0\n",
      "     ResidualAdd-444               [-1, 51, 10]               0\n",
      "       LayerNorm-445               [-1, 51, 10]              20\n",
      "          Linear-446              [-1, 51, 500]           5,500\n",
      "            ReLU-447              [-1, 51, 500]               0\n",
      "         Dropout-448              [-1, 51, 500]               0\n",
      "          Linear-449               [-1, 51, 10]           5,010\n",
      "         Dropout-450               [-1, 51, 10]               0\n",
      "     ResidualAdd-451               [-1, 51, 10]               0\n",
      "       LayerNorm-452               [-1, 51, 10]              20\n",
      "          Linear-453               [-1, 51, 10]             110\n",
      "          Linear-454               [-1, 51, 10]             110\n",
      "          Linear-455               [-1, 51, 10]             110\n",
      "         Dropout-456            [-1, 2, 51, 51]               0\n",
      "          Linear-457               [-1, 51, 10]             110\n",
      "MultiHeadAttention-458               [-1, 51, 10]               0\n",
      "         Dropout-459               [-1, 51, 10]               0\n",
      "     ResidualAdd-460               [-1, 51, 10]               0\n",
      "       LayerNorm-461               [-1, 51, 10]              20\n",
      "          Linear-462              [-1, 51, 500]           5,500\n",
      "            ReLU-463              [-1, 51, 500]               0\n",
      "         Dropout-464              [-1, 51, 500]               0\n",
      "          Linear-465               [-1, 51, 10]           5,010\n",
      "         Dropout-466               [-1, 51, 10]               0\n",
      "     ResidualAdd-467               [-1, 51, 10]               0\n",
      "       LayerNorm-468               [-1, 51, 10]              20\n",
      "          Linear-469               [-1, 51, 10]             110\n",
      "          Linear-470               [-1, 51, 10]             110\n",
      "          Linear-471               [-1, 51, 10]             110\n",
      "         Dropout-472            [-1, 2, 51, 51]               0\n",
      "          Linear-473               [-1, 51, 10]             110\n",
      "MultiHeadAttention-474               [-1, 51, 10]               0\n",
      "         Dropout-475               [-1, 51, 10]               0\n",
      "     ResidualAdd-476               [-1, 51, 10]               0\n",
      "       LayerNorm-477               [-1, 51, 10]              20\n",
      "          Linear-478              [-1, 51, 500]           5,500\n",
      "            ReLU-479              [-1, 51, 500]               0\n",
      "         Dropout-480              [-1, 51, 500]               0\n",
      "          Linear-481               [-1, 51, 10]           5,010\n",
      "         Dropout-482               [-1, 51, 10]               0\n",
      "     ResidualAdd-483               [-1, 51, 10]               0\n",
      "       LayerNorm-484               [-1, 51, 10]              20\n",
      "          Linear-485               [-1, 51, 10]             110\n",
      "          Linear-486               [-1, 51, 10]             110\n",
      "          Linear-487               [-1, 51, 10]             110\n",
      "         Dropout-488            [-1, 2, 51, 51]               0\n",
      "          Linear-489               [-1, 51, 10]             110\n",
      "MultiHeadAttention-490               [-1, 51, 10]               0\n",
      "         Dropout-491               [-1, 51, 10]               0\n",
      "     ResidualAdd-492               [-1, 51, 10]               0\n",
      "       LayerNorm-493               [-1, 51, 10]              20\n",
      "          Linear-494              [-1, 51, 500]           5,500\n",
      "            ReLU-495              [-1, 51, 500]               0\n",
      "         Dropout-496              [-1, 51, 500]               0\n",
      "          Linear-497               [-1, 51, 10]           5,010\n",
      "         Dropout-498               [-1, 51, 10]               0\n",
      "     ResidualAdd-499               [-1, 51, 10]               0\n",
      "       LayerNorm-500               [-1, 51, 10]              20\n",
      "          Linear-501               [-1, 51, 10]             110\n",
      "          Linear-502               [-1, 51, 10]             110\n",
      "          Linear-503               [-1, 51, 10]             110\n",
      "         Dropout-504            [-1, 2, 51, 51]               0\n",
      "          Linear-505               [-1, 51, 10]             110\n",
      "MultiHeadAttention-506               [-1, 51, 10]               0\n",
      "         Dropout-507               [-1, 51, 10]               0\n",
      "     ResidualAdd-508               [-1, 51, 10]               0\n",
      "       LayerNorm-509               [-1, 51, 10]              20\n",
      "          Linear-510              [-1, 51, 500]           5,500\n",
      "            ReLU-511              [-1, 51, 500]               0\n",
      "         Dropout-512              [-1, 51, 500]               0\n",
      "          Linear-513               [-1, 51, 10]           5,010\n",
      "         Dropout-514               [-1, 51, 10]               0\n",
      "     ResidualAdd-515               [-1, 51, 10]               0\n",
      "       LayerNorm-516               [-1, 51, 10]              20\n",
      "          Linear-517               [-1, 51, 10]             110\n",
      "          Linear-518               [-1, 51, 10]             110\n",
      "          Linear-519               [-1, 51, 10]             110\n",
      "         Dropout-520            [-1, 2, 51, 51]               0\n",
      "          Linear-521               [-1, 51, 10]             110\n",
      "MultiHeadAttention-522               [-1, 51, 10]               0\n",
      "         Dropout-523               [-1, 51, 10]               0\n",
      "     ResidualAdd-524               [-1, 51, 10]               0\n",
      "       LayerNorm-525               [-1, 51, 10]              20\n",
      "          Linear-526              [-1, 51, 500]           5,500\n",
      "            ReLU-527              [-1, 51, 500]               0\n",
      "         Dropout-528              [-1, 51, 500]               0\n",
      "          Linear-529               [-1, 51, 10]           5,010\n",
      "         Dropout-530               [-1, 51, 10]               0\n",
      "     ResidualAdd-531               [-1, 51, 10]               0\n",
      "       LayerNorm-532               [-1, 51, 10]              20\n",
      "          Linear-533               [-1, 51, 10]             110\n",
      "          Linear-534               [-1, 51, 10]             110\n",
      "          Linear-535               [-1, 51, 10]             110\n",
      "         Dropout-536            [-1, 2, 51, 51]               0\n",
      "          Linear-537               [-1, 51, 10]             110\n",
      "MultiHeadAttention-538               [-1, 51, 10]               0\n",
      "         Dropout-539               [-1, 51, 10]               0\n",
      "     ResidualAdd-540               [-1, 51, 10]               0\n",
      "       LayerNorm-541               [-1, 51, 10]              20\n",
      "          Linear-542              [-1, 51, 500]           5,500\n",
      "            ReLU-543              [-1, 51, 500]               0\n",
      "         Dropout-544              [-1, 51, 500]               0\n",
      "          Linear-545               [-1, 51, 10]           5,010\n",
      "         Dropout-546               [-1, 51, 10]               0\n",
      "     ResidualAdd-547               [-1, 51, 10]               0\n",
      "       LayerNorm-548               [-1, 51, 10]              20\n",
      "          Linear-549               [-1, 51, 10]             110\n",
      "          Linear-550               [-1, 51, 10]             110\n",
      "          Linear-551               [-1, 51, 10]             110\n",
      "         Dropout-552            [-1, 2, 51, 51]               0\n",
      "          Linear-553               [-1, 51, 10]             110\n",
      "MultiHeadAttention-554               [-1, 51, 10]               0\n",
      "         Dropout-555               [-1, 51, 10]               0\n",
      "     ResidualAdd-556               [-1, 51, 10]               0\n",
      "       LayerNorm-557               [-1, 51, 10]              20\n",
      "          Linear-558              [-1, 51, 500]           5,500\n",
      "            ReLU-559              [-1, 51, 500]               0\n",
      "         Dropout-560              [-1, 51, 500]               0\n",
      "          Linear-561               [-1, 51, 10]           5,010\n",
      "         Dropout-562               [-1, 51, 10]               0\n",
      "     ResidualAdd-563               [-1, 51, 10]               0\n",
      "       LayerNorm-564               [-1, 51, 10]              20\n",
      "          Linear-565               [-1, 51, 10]             110\n",
      "          Linear-566               [-1, 51, 10]             110\n",
      "          Linear-567               [-1, 51, 10]             110\n",
      "         Dropout-568            [-1, 2, 51, 51]               0\n",
      "          Linear-569               [-1, 51, 10]             110\n",
      "MultiHeadAttention-570               [-1, 51, 10]               0\n",
      "         Dropout-571               [-1, 51, 10]               0\n",
      "     ResidualAdd-572               [-1, 51, 10]               0\n",
      "       LayerNorm-573               [-1, 51, 10]              20\n",
      "          Linear-574              [-1, 51, 500]           5,500\n",
      "            ReLU-575              [-1, 51, 500]               0\n",
      "         Dropout-576              [-1, 51, 500]               0\n",
      "          Linear-577               [-1, 51, 10]           5,010\n",
      "         Dropout-578               [-1, 51, 10]               0\n",
      "     ResidualAdd-579               [-1, 51, 10]               0\n",
      "       LayerNorm-580               [-1, 51, 10]              20\n",
      "          Linear-581               [-1, 51, 10]             110\n",
      "          Linear-582               [-1, 51, 10]             110\n",
      "          Linear-583               [-1, 51, 10]             110\n",
      "         Dropout-584            [-1, 2, 51, 51]               0\n",
      "          Linear-585               [-1, 51, 10]             110\n",
      "MultiHeadAttention-586               [-1, 51, 10]               0\n",
      "         Dropout-587               [-1, 51, 10]               0\n",
      "     ResidualAdd-588               [-1, 51, 10]               0\n",
      "       LayerNorm-589               [-1, 51, 10]              20\n",
      "          Linear-590              [-1, 51, 500]           5,500\n",
      "            ReLU-591              [-1, 51, 500]               0\n",
      "         Dropout-592              [-1, 51, 500]               0\n",
      "          Linear-593               [-1, 51, 10]           5,010\n",
      "         Dropout-594               [-1, 51, 10]               0\n",
      "     ResidualAdd-595               [-1, 51, 10]               0\n",
      "       LayerNorm-596               [-1, 51, 10]              20\n",
      "          Linear-597               [-1, 51, 10]             110\n",
      "          Linear-598               [-1, 51, 10]             110\n",
      "          Linear-599               [-1, 51, 10]             110\n",
      "         Dropout-600            [-1, 2, 51, 51]               0\n",
      "          Linear-601               [-1, 51, 10]             110\n",
      "MultiHeadAttention-602               [-1, 51, 10]               0\n",
      "         Dropout-603               [-1, 51, 10]               0\n",
      "     ResidualAdd-604               [-1, 51, 10]               0\n",
      "       LayerNorm-605               [-1, 51, 10]              20\n",
      "          Linear-606              [-1, 51, 500]           5,500\n",
      "            ReLU-607              [-1, 51, 500]               0\n",
      "         Dropout-608              [-1, 51, 500]               0\n",
      "          Linear-609               [-1, 51, 10]           5,010\n",
      "         Dropout-610               [-1, 51, 10]               0\n",
      "     ResidualAdd-611               [-1, 51, 10]               0\n",
      "       LayerNorm-612               [-1, 51, 10]              20\n",
      "          Linear-613               [-1, 51, 10]             110\n",
      "          Linear-614               [-1, 51, 10]             110\n",
      "          Linear-615               [-1, 51, 10]             110\n",
      "         Dropout-616            [-1, 2, 51, 51]               0\n",
      "          Linear-617               [-1, 51, 10]             110\n",
      "MultiHeadAttention-618               [-1, 51, 10]               0\n",
      "         Dropout-619               [-1, 51, 10]               0\n",
      "     ResidualAdd-620               [-1, 51, 10]               0\n",
      "       LayerNorm-621               [-1, 51, 10]              20\n",
      "          Linear-622              [-1, 51, 500]           5,500\n",
      "            ReLU-623              [-1, 51, 500]               0\n",
      "         Dropout-624              [-1, 51, 500]               0\n",
      "          Linear-625               [-1, 51, 10]           5,010\n",
      "         Dropout-626               [-1, 51, 10]               0\n",
      "     ResidualAdd-627               [-1, 51, 10]               0\n",
      "       LayerNorm-628               [-1, 51, 10]              20\n",
      "          Linear-629               [-1, 51, 10]             110\n",
      "          Linear-630               [-1, 51, 10]             110\n",
      "          Linear-631               [-1, 51, 10]             110\n",
      "         Dropout-632            [-1, 2, 51, 51]               0\n",
      "          Linear-633               [-1, 51, 10]             110\n",
      "MultiHeadAttention-634               [-1, 51, 10]               0\n",
      "         Dropout-635               [-1, 51, 10]               0\n",
      "     ResidualAdd-636               [-1, 51, 10]               0\n",
      "       LayerNorm-637               [-1, 51, 10]              20\n",
      "          Linear-638              [-1, 51, 500]           5,500\n",
      "            ReLU-639              [-1, 51, 500]               0\n",
      "         Dropout-640              [-1, 51, 500]               0\n",
      "          Linear-641               [-1, 51, 10]           5,010\n",
      "         Dropout-642               [-1, 51, 10]               0\n",
      "     ResidualAdd-643               [-1, 51, 10]               0\n",
      "       LayerNorm-644               [-1, 51, 10]              20\n",
      "          Linear-645               [-1, 51, 10]             110\n",
      "          Linear-646               [-1, 51, 10]             110\n",
      "          Linear-647               [-1, 51, 10]             110\n",
      "         Dropout-648            [-1, 2, 51, 51]               0\n",
      "          Linear-649               [-1, 51, 10]             110\n",
      "MultiHeadAttention-650               [-1, 51, 10]               0\n",
      "         Dropout-651               [-1, 51, 10]               0\n",
      "     ResidualAdd-652               [-1, 51, 10]               0\n",
      "       LayerNorm-653               [-1, 51, 10]              20\n",
      "          Linear-654              [-1, 51, 500]           5,500\n",
      "            ReLU-655              [-1, 51, 500]               0\n",
      "         Dropout-656              [-1, 51, 500]               0\n",
      "          Linear-657               [-1, 51, 10]           5,010\n",
      "         Dropout-658               [-1, 51, 10]               0\n",
      "     ResidualAdd-659               [-1, 51, 10]               0\n",
      "       LayerNorm-660               [-1, 51, 10]              20\n",
      "          Linear-661               [-1, 51, 10]             110\n",
      "          Linear-662               [-1, 51, 10]             110\n",
      "          Linear-663               [-1, 51, 10]             110\n",
      "         Dropout-664            [-1, 2, 51, 51]               0\n",
      "          Linear-665               [-1, 51, 10]             110\n",
      "MultiHeadAttention-666               [-1, 51, 10]               0\n",
      "         Dropout-667               [-1, 51, 10]               0\n",
      "     ResidualAdd-668               [-1, 51, 10]               0\n",
      "       LayerNorm-669               [-1, 51, 10]              20\n",
      "          Linear-670              [-1, 51, 500]           5,500\n",
      "            ReLU-671              [-1, 51, 500]               0\n",
      "         Dropout-672              [-1, 51, 500]               0\n",
      "          Linear-673               [-1, 51, 10]           5,010\n",
      "         Dropout-674               [-1, 51, 10]               0\n",
      "     ResidualAdd-675               [-1, 51, 10]               0\n",
      "       LayerNorm-676               [-1, 51, 10]              20\n",
      "          Linear-677               [-1, 51, 10]             110\n",
      "          Linear-678               [-1, 51, 10]             110\n",
      "          Linear-679               [-1, 51, 10]             110\n",
      "         Dropout-680            [-1, 2, 51, 51]               0\n",
      "          Linear-681               [-1, 51, 10]             110\n",
      "MultiHeadAttention-682               [-1, 51, 10]               0\n",
      "         Dropout-683               [-1, 51, 10]               0\n",
      "     ResidualAdd-684               [-1, 51, 10]               0\n",
      "       LayerNorm-685               [-1, 51, 10]              20\n",
      "          Linear-686              [-1, 51, 500]           5,500\n",
      "            ReLU-687              [-1, 51, 500]               0\n",
      "         Dropout-688              [-1, 51, 500]               0\n",
      "          Linear-689               [-1, 51, 10]           5,010\n",
      "         Dropout-690               [-1, 51, 10]               0\n",
      "     ResidualAdd-691               [-1, 51, 10]               0\n",
      "       LayerNorm-692               [-1, 51, 10]              20\n",
      "          Linear-693               [-1, 51, 10]             110\n",
      "          Linear-694               [-1, 51, 10]             110\n",
      "          Linear-695               [-1, 51, 10]             110\n",
      "         Dropout-696            [-1, 2, 51, 51]               0\n",
      "          Linear-697               [-1, 51, 10]             110\n",
      "MultiHeadAttention-698               [-1, 51, 10]               0\n",
      "         Dropout-699               [-1, 51, 10]               0\n",
      "     ResidualAdd-700               [-1, 51, 10]               0\n",
      "       LayerNorm-701               [-1, 51, 10]              20\n",
      "          Linear-702              [-1, 51, 500]           5,500\n",
      "            ReLU-703              [-1, 51, 500]               0\n",
      "         Dropout-704              [-1, 51, 500]               0\n",
      "          Linear-705               [-1, 51, 10]           5,010\n",
      "         Dropout-706               [-1, 51, 10]               0\n",
      "     ResidualAdd-707               [-1, 51, 10]               0\n",
      "       LayerNorm-708               [-1, 51, 10]              20\n",
      "          Linear-709               [-1, 51, 10]             110\n",
      "          Linear-710               [-1, 51, 10]             110\n",
      "          Linear-711               [-1, 51, 10]             110\n",
      "         Dropout-712            [-1, 2, 51, 51]               0\n",
      "          Linear-713               [-1, 51, 10]             110\n",
      "MultiHeadAttention-714               [-1, 51, 10]               0\n",
      "         Dropout-715               [-1, 51, 10]               0\n",
      "     ResidualAdd-716               [-1, 51, 10]               0\n",
      "       LayerNorm-717               [-1, 51, 10]              20\n",
      "          Linear-718              [-1, 51, 500]           5,500\n",
      "            ReLU-719              [-1, 51, 500]               0\n",
      "         Dropout-720              [-1, 51, 500]               0\n",
      "          Linear-721               [-1, 51, 10]           5,010\n",
      "         Dropout-722               [-1, 51, 10]               0\n",
      "     ResidualAdd-723               [-1, 51, 10]               0\n",
      "       LayerNorm-724               [-1, 51, 10]              20\n",
      "          Linear-725               [-1, 51, 10]             110\n",
      "          Linear-726               [-1, 51, 10]             110\n",
      "          Linear-727               [-1, 51, 10]             110\n",
      "         Dropout-728            [-1, 2, 51, 51]               0\n",
      "          Linear-729               [-1, 51, 10]             110\n",
      "MultiHeadAttention-730               [-1, 51, 10]               0\n",
      "         Dropout-731               [-1, 51, 10]               0\n",
      "     ResidualAdd-732               [-1, 51, 10]               0\n",
      "       LayerNorm-733               [-1, 51, 10]              20\n",
      "          Linear-734              [-1, 51, 500]           5,500\n",
      "            ReLU-735              [-1, 51, 500]               0\n",
      "         Dropout-736              [-1, 51, 500]               0\n",
      "          Linear-737               [-1, 51, 10]           5,010\n",
      "         Dropout-738               [-1, 51, 10]               0\n",
      "     ResidualAdd-739               [-1, 51, 10]               0\n",
      "       LayerNorm-740               [-1, 51, 10]              20\n",
      "          Linear-741               [-1, 51, 10]             110\n",
      "          Linear-742               [-1, 51, 10]             110\n",
      "          Linear-743               [-1, 51, 10]             110\n",
      "         Dropout-744            [-1, 2, 51, 51]               0\n",
      "          Linear-745               [-1, 51, 10]             110\n",
      "MultiHeadAttention-746               [-1, 51, 10]               0\n",
      "         Dropout-747               [-1, 51, 10]               0\n",
      "     ResidualAdd-748               [-1, 51, 10]               0\n",
      "       LayerNorm-749               [-1, 51, 10]              20\n",
      "          Linear-750              [-1, 51, 500]           5,500\n",
      "            ReLU-751              [-1, 51, 500]               0\n",
      "         Dropout-752              [-1, 51, 500]               0\n",
      "          Linear-753               [-1, 51, 10]           5,010\n",
      "         Dropout-754               [-1, 51, 10]               0\n",
      "     ResidualAdd-755               [-1, 51, 10]               0\n",
      "       LayerNorm-756               [-1, 51, 10]              20\n",
      "          Linear-757               [-1, 51, 10]             110\n",
      "          Linear-758               [-1, 51, 10]             110\n",
      "          Linear-759               [-1, 51, 10]             110\n",
      "         Dropout-760            [-1, 2, 51, 51]               0\n",
      "          Linear-761               [-1, 51, 10]             110\n",
      "MultiHeadAttention-762               [-1, 51, 10]               0\n",
      "         Dropout-763               [-1, 51, 10]               0\n",
      "     ResidualAdd-764               [-1, 51, 10]               0\n",
      "       LayerNorm-765               [-1, 51, 10]              20\n",
      "          Linear-766              [-1, 51, 500]           5,500\n",
      "            ReLU-767              [-1, 51, 500]               0\n",
      "         Dropout-768              [-1, 51, 500]               0\n",
      "          Linear-769               [-1, 51, 10]           5,010\n",
      "         Dropout-770               [-1, 51, 10]               0\n",
      "     ResidualAdd-771               [-1, 51, 10]               0\n",
      "       LayerNorm-772               [-1, 51, 10]              20\n",
      "          Linear-773               [-1, 51, 10]             110\n",
      "          Linear-774               [-1, 51, 10]             110\n",
      "          Linear-775               [-1, 51, 10]             110\n",
      "         Dropout-776            [-1, 2, 51, 51]               0\n",
      "          Linear-777               [-1, 51, 10]             110\n",
      "MultiHeadAttention-778               [-1, 51, 10]               0\n",
      "         Dropout-779               [-1, 51, 10]               0\n",
      "     ResidualAdd-780               [-1, 51, 10]               0\n",
      "       LayerNorm-781               [-1, 51, 10]              20\n",
      "          Linear-782              [-1, 51, 500]           5,500\n",
      "            ReLU-783              [-1, 51, 500]               0\n",
      "         Dropout-784              [-1, 51, 500]               0\n",
      "          Linear-785               [-1, 51, 10]           5,010\n",
      "         Dropout-786               [-1, 51, 10]               0\n",
      "     ResidualAdd-787               [-1, 51, 10]               0\n",
      "       LayerNorm-788               [-1, 51, 10]              20\n",
      "          Linear-789               [-1, 51, 10]             110\n",
      "          Linear-790               [-1, 51, 10]             110\n",
      "          Linear-791               [-1, 51, 10]             110\n",
      "         Dropout-792            [-1, 2, 51, 51]               0\n",
      "          Linear-793               [-1, 51, 10]             110\n",
      "MultiHeadAttention-794               [-1, 51, 10]               0\n",
      "         Dropout-795               [-1, 51, 10]               0\n",
      "     ResidualAdd-796               [-1, 51, 10]               0\n",
      "       LayerNorm-797               [-1, 51, 10]              20\n",
      "          Linear-798              [-1, 51, 500]           5,500\n",
      "            ReLU-799              [-1, 51, 500]               0\n",
      "         Dropout-800              [-1, 51, 500]               0\n",
      "          Linear-801               [-1, 51, 10]           5,010\n",
      "         Dropout-802               [-1, 51, 10]               0\n",
      "     ResidualAdd-803               [-1, 51, 10]               0\n",
      "          Reduce-804                   [-1, 10]               0\n",
      "       LayerNorm-805                   [-1, 10]              20\n",
      "          Linear-806                    [-1, 1]              11\n",
      "================================================================\n",
      "Total params: 549,641\n",
      "Trainable params: 549,641\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 33.51\n",
      "Params size (MB): 2.10\n",
      "Estimated Total Size (MB): 35.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ViT(), (1, 500, 1), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5340c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples : 120000000\n",
      "validation samples : 40000000\n",
      "testing samples : 40000000\n"
     ]
    }
   ],
   "source": [
    "train_X = np.load('train_X.npy').flatten()\n",
    "train_y_r = np.sum(np.load('train_Y.npy'),axis=1).flatten()\n",
    "\n",
    "test_X = np.load('test_X.npy').flatten()\n",
    "test_y_r = np.sum(np.load('test_y.npy'),axis=1).flatten()\n",
    "\n",
    "val_X = np.load('val_X.npy').flatten()\n",
    "val_y_r = np.sum(np.load('val_y.npy'),axis=1).flatten()\n",
    "\n",
    "# training\n",
    "#train_X = train_X.reshape(1,np.int(train_X.shape[0]/1000),1000, 1)\n",
    "#train_y_r = train_y_r.reshape(train_y_r.shape[0],1)\n",
    "#train_y_pp = train_y_pp.reshape(np.int(train_y_pp.shape[0]/100),100,1)\n",
    "\n",
    "# validation\n",
    "#val_X = val_X.reshape(np.int(val_X.shape[0]/1000),1000, 1)\n",
    "#val_y_r = val_y_r.reshape(val_y_r.shape[0],1)\n",
    "#val_y_pp = val_y_pp.reshape(np.int(val_y_pp.shape[0]/100),100,1)\n",
    "\n",
    "print(\"training samples :\",train_X.shape[0])\n",
    "print(\"validation samples :\",val_X.shape[0])\n",
    "print(\"testing samples :\",test_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c0f100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "x = torch.from_numpy(train_X[:10000000].astype(np.float32))#.double()\n",
    "x = torch.reshape(x,(10000,1,1000,1))#.double()\n",
    "y = torch.from_numpy(train_y_r[:10000].astype(np.float32))\n",
    "c = 1; h=1000; w=1\n",
    "x_train = torch.Tensor(x).reshape(x.shape[0],c,h,w)\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "vx = torch.from_numpy(val_X[:8000000].astype(np.float32))#.double()\n",
    "vx = torch.reshape(vx,(8000,1,1000,1))#.double()\n",
    "vy = torch.from_numpy(train_y_r[:8000].astype(np.float32))\n",
    "c = 1; h=1000; w=1\n",
    "v_train = torch.Tensor(vx).reshape(vx.shape[0],c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9609a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "x = torch.from_numpy(train_X[:].astype(np.float32))#.double()\n",
    "x = torch.reshape(x,(240000,1,500,1))#.double()\n",
    "y = torch.from_numpy(train_y_r[:].astype(np.float32))\n",
    "c = 1; h=500; w=1\n",
    "x_train = torch.Tensor(x).reshape(x.shape[0],c,h,w)\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "vx = torch.from_numpy(val_X[:].astype(np.float32))#.double()\n",
    "vx = torch.reshape(vx,(80000,1,500,1))#.double()\n",
    "vy = torch.from_numpy(val_y_r[:].astype(np.float32))\n",
    "c = 1; h=500; w=1\n",
    "v_train = torch.Tensor(vx).reshape(vx.shape[0],c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "371a9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([240000, 1, 500, 1]) - y_train: torch.Size([240000])\n",
      "x_train shape: torch.Size([80000, 1, 500, 1]) - y_train: torch.Size([80000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1364/2834397934.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# update the model parameters by performing a single optimization step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# accumulate the training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    148\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    151\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    205\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = torch.Tensor(y).reshape(y.shape[0],)#.type(torch.LongTensor)\n",
    "vy_train = torch.Tensor(vy).reshape(vy.shape[0],)#.type(torch.LongTensor)\n",
    "#y_test = torch.Tensor(y_test).reshape(y_test.shape[0],)\n",
    "print(f'x_train shape: {x_train.shape} - y_train: {y_train.shape}')\n",
    "print(f'x_train shape: {v_train.shape} - y_train: {vy_train.shape}')\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set,batch_size=128)\n",
    "\n",
    "valid_set = TensorDataset(v_train,vy_train)\n",
    "validloader = DataLoader(valid_set,batch_size=128)\n",
    "\n",
    "from torch import optim\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay= 1e-3, momentum = 0.6, nesterov = True)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "n_epochs = 50 # this is a hyperparameter you'll need to define\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##################\n",
    "    ### TRAIN LOOP ###\n",
    "    ##################\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the old gradients from optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: feed inputs to the model to get outputs\n",
    "        output = model(data)\n",
    "        # calculate the training batch loss\n",
    "        loss = loss_function(output, target)\n",
    "        # backward: perform gradient descent of the loss w.r. to the model params\n",
    "        loss.backward()\n",
    "        # update the model parameters by performing a single optimization step\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #######################\n",
    "    ### VALIDATION LOOP ###\n",
    "    #######################\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for data, target in validloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # validation batch loss\n",
    "            loss = loss_function(output, target) \n",
    "            # accumulate the valid_loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    #########################\n",
    "    ## PRINT EPOCH RESULTS ##\n",
    "    #########################\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validloader)\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643ff05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3db2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fd5f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([161000, 1, 1000, 1]) - y_train: torch.Size([161000])\n",
      "x_train shape: torch.Size([8000, 1, 1000, 1]) - y_train: torch.Size([8000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jigar\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/1953217474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# forward pass: feed inputs to the model to get outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m# calculate the training batch loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/2857984585.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mx\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/2857984585.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b n (h d) -> b h n d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#print('key : ',keys.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mvalues\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b n (h d) -> b h n d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m#print('values : ',values.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# sum up over the last axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = torch.Tensor(y).reshape(y.shape[0],)#.type(torch.LongTensor)\n",
    "vy_train = torch.Tensor(vy).reshape(vy.shape[0],)#.type(torch.LongTensor)\n",
    "#y_test = torch.Tensor(y_test).reshape(y_test.shape[0],)\n",
    "print(f'x_train shape: {x_train.shape} - y_train: {y_train.shape}')\n",
    "print(f'x_train shape: {v_train.shape} - y_train: {vy_train.shape}')\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set,batch_size=1000)\n",
    "\n",
    "valid_set = TensorDataset(v_train,vy_train)\n",
    "validloader = DataLoader(valid_set,batch_size=1000)\n",
    "\n",
    "from torch import optim\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay= 1e-3, momentum = 0.6, nesterov = True)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "n_epochs = 50 # this is a hyperparameter you'll need to define\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##################\n",
    "    ### TRAIN LOOP ###\n",
    "    ##################\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        # clear the old gradients from optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: feed inputs to the model to get outputs\n",
    "        output = model(data)\n",
    "        # calculate the training batch loss\n",
    "        loss = loss_function(output, target)\n",
    "        # backward: perform gradient descent of the loss w.r. to the model params\n",
    "        loss.backward()\n",
    "        # update the model parameters by performing a single optimization step\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #######################\n",
    "    ### VALIDATION LOOP ###\n",
    "    #######################\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for data, target in validloader:\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # validation batch loss\n",
    "            loss = loss_function(output, target) \n",
    "            # accumulate the valid_loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    #########################\n",
    "    ## PRINT EPOCH RESULTS ##\n",
    "    #########################\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validloader)\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad8577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937bd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba1a6585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.cuda.device_count()\n",
    "\n",
    "torch.cuda.current_device()\n",
    "\n",
    "torch.cuda.device(0)\n",
    "\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2f9945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f000a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b663505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ac5aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([161000, 1, 1000, 1]) - y_train: torch.Size([161000])\n",
      "x_train shape: torch.Size([8000, 1, 1000, 1]) - y_train: torch.Size([8000])\n",
      "Epoch: 1/50.. Training loss: 6461.130845883248.. Validation Loss: 362.01186180114746\n",
      "Epoch: 2/50.. Training loss: 4439.795080212332.. Validation Loss: 1370.8190307617188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11368/3640602335.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# update the model parameters by performing a single optimization step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;31m# accumulate the training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    148\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    151\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    205\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mstep_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# update step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mstep_t\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_train = torch.Tensor(y).reshape(y.shape[0],)#.type(torch.LongTensor)\n",
    "vy_train = torch.Tensor(vy).reshape(vy.shape[0],)#.type(torch.LongTensor)\n",
    "#y_test = torch.Tensor(y_test).reshape(y_test.shape[0],)\n",
    "print(f'x_train shape: {x_train.shape} - y_train: {y_train.shape}')\n",
    "print(f'x_train shape: {v_train.shape} - y_train: {vy_train.shape}')\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_set,batch_size=1000)\n",
    "\n",
    "valid_set = TensorDataset(v_train,vy_train)\n",
    "validloader = DataLoader(valid_set,batch_size=1000)\n",
    "\n",
    "from torch import optim\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay= 1e-3, momentum = 0.6, nesterov = True)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "n_epochs = 50 # this is a hyperparameter you'll need to define\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ##################\n",
    "    ### TRAIN LOOP ###\n",
    "    ##################\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the old gradients from optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: feed inputs to the model to get outputs\n",
    "        output = model(data)\n",
    "        # calculate the training batch loss\n",
    "        loss = loss_function(output, target)\n",
    "        # backward: perform gradient descent of the loss w.r. to the model params\n",
    "        loss.backward()\n",
    "        # update the model parameters by performing a single optimization step\n",
    "        optimizer.step()\n",
    "        # accumulate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #######################\n",
    "    ### VALIDATION LOOP ###\n",
    "    #######################\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    # turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for data, target in validloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # validation batch loss\n",
    "            loss = loss_function(output, target) \n",
    "            # accumulate the valid_loss\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    #########################\n",
    "    ## PRINT EPOCH RESULTS ##\n",
    "    #########################\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validloader)\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18776bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
